{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "from nltk import bigrams, ConditionalFreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "from nltk import bigrams, ConditionalFreqDist\n",
    "\n",
    "sentences = brown.sents()\n",
    "\n",
    "tagged_sentences = [nltk.pos_tag(sentence) for sentence in sentences]\n",
    "\n",
    "# Convert the sentences to sequences of POS tags\n",
    "# tag_sequences = [[tag for word, tag in sentence] for sentence in tagged_sentences]\n",
    "\n",
    "# # Generate bigrams from these sequences\n",
    "# tag_bigrams = [bigram for sent in tag_sequences for bigram in bigrams(sent)]\n",
    "\n",
    "# # Train a conditional frequency distribution model on these bigrams\n",
    "# cfd = ConditionalFreqDist(tag_bigrams)\n",
    "\n",
    "# # Define a function to calculate the probability of a tag given the previous tag\n",
    "# def tag_probability(prev_tag, current_tag):\n",
    "#     # Count of current tag following prev_tag\n",
    "#     current_tag_count = cfd[prev_tag][current_tag]\n",
    "#     # Total count of all tags following prev_tag\n",
    "#     total_count = sum(cfd[prev_tag].values())\n",
    "#     probability = current_tag_count / total_count if total_count > 0 else 0\n",
    "#     return probability\n",
    "\n",
    "# # Example usage: Probability of a noun (NOUN) following a determiner (DET)\n",
    "# print(tag_probability('DET', 'NOUN'))\n",
    "# print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.646136789387162e-05\n"
     ]
    }
   ],
   "source": [
    "tag_sequences = [[tag for word, tag in sentence] for sentence in tagged_sentences]\n",
    "tag_bigrams = [bigram for sent in tag_sequences for bigram in bigrams(sent)]\n",
    "cfd = ConditionalFreqDist(tag_bigrams)\n",
    "\n",
    "# Define a function to calculate the probability of a tag given the previous tag\n",
    "def tag_probability(prev_tag, current_tag):\n",
    "    # Count of current tag following prev_tag\n",
    "    current_tag_count = cfd[prev_tag][current_tag]\n",
    "    # Total count of all tags following prev_tag\n",
    "    total_count = sum(cfd[prev_tag].values())\n",
    "    probability = current_tag_count / total_count if total_count > 0 else 0\n",
    "    return probability\n",
    "\n",
    "# Example usage: Probability of a noun (NOUN) following a determiner (DET)\n",
    "print(tag_probability('TO', 'VBP'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the cfd object to a file\n",
    "with open('cfd.pkl', 'wb') as f:\n",
    "    pickle.dump(cfd, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Load the cfd object from the file\n",
    "with open('cfd.pkl', 'rb') as f:\n",
    "    cfd = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.646136789387162e-05\n"
     ]
    }
   ],
   "source": [
    "# Define a function to calculate the probability of a tag given the previous tag\n",
    "def tag_probability(prev_tag, current_tag):\n",
    "    # Count of current tag following prev_tag\n",
    "    current_tag_count = cfd[prev_tag][current_tag]\n",
    "    # Total count of all tags following prev_tag\n",
    "    total_count = sum(cfd[prev_tag].values())\n",
    "    probability = current_tag_count / total_count if total_count > 0 else 0\n",
    "    return probability\n",
    "\n",
    "# Example usage: Probability of a noun (NOUN) following a determiner (DET)\n",
    "print(tag_probability('TO', 'VBP'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('to', 'TO'), ('be', 'VB')]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(['to', 'be',])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('i', 'UNK'), ('am', 'UNK'), ('sun', 'UNK')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "def get_pos_tags(text):\n",
    "    \"\"\"\n",
    "    Tokenize the input text and return POS tags for each token.\n",
    "    \n",
    "    Parameters:\n",
    "    - text (str): The input text to tag.\n",
    "    \n",
    "    Returns:\n",
    "    - list of tuples: A list where each tuple contains a token and its POS tag.\n",
    "    \"\"\"\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    # Tag the tokens with POS tags\n",
    "    tagged_tokens = pos_tag(tokens, tagset='brown')\n",
    "    return tagged_tokens\n",
    "\n",
    "# Example text\n",
    "text = \"i am sun\"\n",
    "\n",
    "# Get POS tags for the example text\n",
    "tags = get_pos_tags(text)\n",
    "print(tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6337448559670782\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from collections import Counter\n",
    "\n",
    "# Load the English language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Load the POS-tagged sentences from the Brown Corpus using spaCy\n",
    "brown_corpus_sentences = brown.sents(categories='news')[:100]  # Using a subset of sentences for demonstration\n",
    "tagged_sentences = []\n",
    "for sent in brown_corpus_sentences:\n",
    "    doc = nlp(\" \".join(sent))\n",
    "    tagged_sentences.append([(token.text, token.pos_) for token in doc])\n",
    "\n",
    "# Convert the sentences to sequences of POS tags\n",
    "tag_sequences = [[tag for word, tag in sentence] for sentence in tagged_sentences]\n",
    "\n",
    "# Generate bigrams from these sequences\n",
    "tag_bigrams = [bigram for sent in tag_sequences for bigram in zip(sent[:-1], sent[1:])]\n",
    "\n",
    "# Count occurrences of bigrams\n",
    "cfd = Counter(tag_bigrams)\n",
    "\n",
    "# Define a function to calculate the probability of a tag given the previous tag\n",
    "def tag_probability(prev_tag, current_tag):\n",
    "    # Count of current tag following prev_tag\n",
    "    current_tag_count = cfd[(prev_tag, current_tag)]\n",
    "    # Total count of all tags following prev_tag\n",
    "    total_count = sum(cfd[bigram] for bigram in cfd if bigram[0] == prev_tag)\n",
    "    probability = current_tag_count / total_count if total_count > 0 else 0\n",
    "    return probability\n",
    "\n",
    "# Example usage: Probability of a noun (NOUN) following a determiner (DET)\n",
    "print(tag_probability('DET', 'NOUN'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'list'>, {'DET': ['ADJ', 'X', 'SYM', 'AUX', 'INTJ', 'NOUN', 'VERB', 'NUM', 'PRON', 'ADP', 'PROPN', 'PART', 'ADV', 'SCONJ', 'PUNCT', 'DET', 'CCONJ'], 'PROPN': ['ADJ', 'X', 'SYM', 'AUX', 'INTJ', 'NOUN', 'VERB', 'NUM', 'PRON', 'ADP', 'PROPN', 'PART', 'ADV', 'SCONJ', 'PUNCT', 'DET', 'CCONJ'], 'VERB': ['ADJ', 'X', 'SYM', 'AUX', 'INTJ', 'NOUN', 'VERB', 'NUM', 'PRON', 'ADP', 'PROPN', 'PART', 'ADV', 'SCONJ', 'PUNCT', 'DET', 'CCONJ'], 'NOUN': ['ADJ', 'X', 'SYM', 'AUX', 'INTJ', 'NOUN', 'VERB', 'NUM', 'PRON', 'ADP', 'PROPN', 'PART', 'ADV', 'SCONJ', 'PUNCT', 'DET', 'CCONJ'], 'ADP': ['ADJ', 'X', 'SYM', 'AUX', 'INTJ', 'NOUN', 'VERB', 'NUM', 'PRON', 'ADP', 'PROPN', 'PART', 'ADV', 'SCONJ', 'PUNCT', 'DET', 'CCONJ'], 'PART': ['ADJ', 'SYM', 'AUX', 'INTJ', 'NOUN', 'VERB', 'NUM', 'PRON', 'ADP', 'PROPN', 'PART', 'ADV', 'SCONJ', 'PUNCT', 'DET', 'CCONJ'], 'ADJ': ['ADJ', 'X', 'SYM', 'AUX', 'INTJ', 'NOUN', 'VERB', 'NUM', 'PRON', 'ADP', 'PROPN', 'PART', 'ADV', 'SCONJ', 'PUNCT', 'DET', 'CCONJ'], 'PUNCT': ['ADJ', 'X', 'SYM', 'AUX', 'INTJ', 'NOUN', 'VERB', 'NUM', 'PRON', 'ADP', 'PROPN', 'PART', 'ADV', 'SCONJ', 'PUNCT', 'DET', 'CCONJ'], 'SCONJ': ['ADJ', 'SYM', 'AUX', 'INTJ', 'NOUN', 'VERB', 'NUM', 'PRON', 'ADP', 'PROPN', 'PART', 'ADV', 'SCONJ', 'PUNCT', 'DET', 'CCONJ'], 'ADV': ['ADJ', 'X', 'SYM', 'AUX', 'INTJ', 'NOUN', 'VERB', 'NUM', 'PRON', 'ADP', 'PROPN', 'PART', 'ADV', 'SCONJ', 'PUNCT', 'DET', 'CCONJ'], 'PRON': ['ADJ', 'X', 'SYM', 'AUX', 'INTJ', 'NOUN', 'VERB', 'NUM', 'PRON', 'ADP', 'PROPN', 'PART', 'ADV', 'SCONJ', 'PUNCT', 'DET', 'CCONJ'], 'CCONJ': ['ADJ', 'X', 'SYM', 'AUX', 'INTJ', 'NOUN', 'VERB', 'NUM', 'PRON', 'ADP', 'PROPN', 'PART', 'ADV', 'SCONJ', 'PUNCT', 'DET', 'CCONJ'], 'AUX': ['ADJ', 'SYM', 'AUX', 'INTJ', 'NOUN', 'VERB', 'NUM', 'PRON', 'ADP', 'PROPN', 'PART', 'ADV', 'SCONJ', 'PUNCT', 'DET', 'CCONJ'], 'NUM': ['ADJ', 'X', 'SYM', 'AUX', 'NOUN', 'VERB', 'NUM', 'PRON', 'ADP', 'PROPN', 'PART', 'ADV', 'SCONJ', 'PUNCT', 'DET', 'CCONJ'], 'X': ['ADJ', 'X', 'AUX', 'NOUN', 'VERB', 'PRON', 'ADP', 'PROPN', 'ADV', 'SCONJ', 'PUNCT', 'DET', 'CCONJ'], 'INTJ': ['ADJ', 'AUX', 'INTJ', 'NOUN', 'VERB', 'NUM', 'PRON', 'ADP', 'PROPN', 'PART', 'ADV', 'SCONJ', 'PUNCT', 'DET', 'CCONJ'], 'SYM': ['ADJ', 'X', 'AUX', 'NOUN', 'NUM', 'ADP', 'PROPN', 'ADV']})\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "\n",
    "# Download the Brown Corpus if it's not already downloaded\n",
    "# nltk.download('brown')\n",
    "\n",
    "# Load the spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Load the Brown Corpus as a list of sentences\n",
    "sentences = brown.sents()\n",
    "\n",
    "# Initialize a dictionary to hold our sequence data\n",
    "# Using defaultdict to automatically initialize unseen keys with an empty list\n",
    "pos_sequences = defaultdict(list)\n",
    "\n",
    "# Process each sentence with spaCy\n",
    "for sentence in sentences:\n",
    "    # Join the words in the sentence into a single string\n",
    "    text = ' '.join(sentence)\n",
    "    # Process the text with spaCy\n",
    "    doc = nlp(text)\n",
    "    # Iterate through the tokens in the doc\n",
    "    for token, next_token in zip(doc[:-1], doc[1:]):\n",
    "        # Append the POS of the next token to the list of the current token's POS\n",
    "        pos_sequences[token.pos_].append(next_token.pos_)\n",
    "        \n",
    "# If you want each POS tag to map to a unique list of following POS tags:\n",
    "for pos in pos_sequences:\n",
    "    pos_sequences[pos] = list(set(pos_sequences[pos]))\n",
    "\n",
    "print(pos_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# ... (rest of your code)\n",
    "\n",
    "# Save the pos_sequences dictionary to a JSON file\n",
    "with open('pos_sequences.json', 'w') as f:\n",
    "    json.dump(pos_sequences, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i PRON\n",
      "want VERB\n",
      "be AUX\n",
      "to ADP\n",
      "a DET\n",
      "good ADJ\n",
      "person NOUN\n"
     ]
    }
   ],
   "source": [
    "text = \"i want be to a good person\"\n",
    "doc = nlp(text)\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
